---
title: "Introduction to Open Data Science: Rstudio Exercise 2"
output: html_document
---

```{r setup, echo=FALSE, message=FALSE}
# Clear memory.
rm(list = ls())

# Define packages required by this script.
library(dplyr)
library(GGally)
library(ggplot2)

# This affects wrappings of the output of summary().
options(width = 60)
```

# Analysis of the Learning Questionnaire

<!-- *Describe the work you have done this week and summarize your learning.*

- Describe your work and results clearly. 
- Assume the reader has an introductory course level understanding of writing and reading R code as well as statistical methods
- Assume the reader has no previous knowledge of your data or the more advanced methods you are using  
-->

## Introduction

The data of the analysis in this exercise was the Autumn 2014 Introductory Statistics Course Learning Questionnaire. The data has 60 variables and 183 observations; except for the few background related variables (age, attitude towards statistics, course points), most of the questions were learning-related questions with answers given on Likert scale, from 1 to 5.

For the analysis, I've averaged the variables related to deep learning, surface learning, and strategic learning. (For the initial data wrangling part of this exercise, see [this R script](https://github.com/pinjaliina/IODS-project/blob/master/data/create_learning2014.R))

**Note: it seems that the chosen theme of my course diary hides embedded R code by default, even if ```echo``` is set to ```TRUE```. If you can't see the code, open the code menu from the top right corner of the diary and click “Show All Code”.**

## Overview of the Data

The data is read in as follows:
```{r data_input}
# Read in the data
lrn2014a <- as.data.frame(read.table('data/learning2014.csv',  sep="\t", header=TRUE))
```

A scatter plot matrix of the variables of the data can be drawn as follows, coloured by the gender variable:
```{r scatter_plot_matrices, fig.width=10, fig.height=10}
p <- ggpairs(lrn2014a, mapping = aes(col = gender), lower = list(combo = wrap("facethist", bins = 20)))
p
```

A summary of each of the variables of the data can be displayed as follows:
```{r data_summary}
summary(lrn2014a)
```

As can be seen from the output, most of the variables – with the exception of the age of the students – are distributed quite randomly and only correlate weakly with the course points. The most significant exception is the attitude towards statistics, which correlates more strongly with the course points than any other variable. Except for the age and points, the distribution of most of the variables seems to be reasonably close to normal distribution. The gender variable demonstrates that the course was attended by significantly more female than male students.

## Fitting of a Regression Model

A linear regression model can be fit to the data as follows:
```{r lm}
lrn2014_model <- lm(points ~ attitude + stra + surf, data = lrn2014a)
```

The chosen explanatory variables for the model were attitude, strategic learning and surface learning. The summary of the model can be printed as follows:
```{r lm_summary}
summary(lrn2014_model)
```

However, as can be seen from the model summary, the estimates of strategic and surface learning have no statistical significance explaining the course points; given the weak correlation of those variables with the points this was somewhat expected. It thus makes more sense to eliminate at least the variable that has the lowest probability value, strategic learning:
```{r final_lm_and_summary}
lrn2014_model <- lm(points ~ attitude + surf, data = lrn2014a)
summary(lrn2014_model)
```

## Analysis of the Summary

With the removal of that value the statistical significance of the surface learning estimates improves somewhat to near significant levels and can be left to the model. In practice its effect to the model is so tiny that it nearly as well be left out, but because it is reasonably close to statistical significance *and* also gives highest adjusted R<sup>2</sup> value – the adjusted R<sup>2</sup> of a model where attitude is the only explanatory variable as reported by ```summary()``` would be `r round(summary(lm(points ~ attitude, data = lrn2014a))$adj.r.squared, digits = 4)` – its inclusion to the model is justified.

While the model estimates are statistically significant, the model as a whole is not very good: the multiple R<sup>2</sup> value is only 0.20, which means that about 80 % of the relationship between the dependent variable – course points – and the explanatory variables remains unexplained. Thus, any predictions based on the model alone might not be very reliable.

## Diagnostic Plots and Assumptions of the Model

In addition to linearity, linear models are fitted with the assumption that:

1. The errors of the model are normally distributed.
2. The errors are not correlated.
3. The size of the errors does not depend on the explanatory variables.

The validity of these assumption can be tested by analysing the residuals of the model. This can be done with the help of different kinds of diagnostic plots. In the following figure three different plots are drawn:

* A residuals vs. fitted values plot
* A Q–Q-plot
* A Residuals vs. leverage plot

```{r diag_plots, fig.width=10, fig.height=3.5}
par(mfrow = c(1,3)) # Set some graphical params.
# It seems that the drawing order of the plot is independent of the vector
# order and can't be changed.
plot(lrn2014_model, which = c(1,2,5))
```

Interpretation of the plots:

1. The Q–Q-plot demonstrates that the standardised residuals of the model fit to the theoretical quantities reasonably well, so the normal distribution assumption is valid.
2. The residuals vs. fitted values plot doesn't show any kind of pattern, so the errors are not correlated and their size is independent of the explanatory variables (their σ<sup>2</sup> is constant)
3. In addition to checking the abovementioned assumptions, it is recommended to check that no single observation has an oversized effect on the model, because these might distort the model coefficients. As the x-axis scale of the residuals vs. leverage plot is relatively narrow with no significant outliers, we can conclude that the model is not distorted by any single observation.